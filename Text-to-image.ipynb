{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c831b5b-3025-4177-bef5-25aaec89573a",
   "metadata": {},
   "source": [
    "# **Running, monitoring and evaluating a training job**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9a96d-668e-40e2-bb63-a24b9213948d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### About this interactive guide\n",
    "\n",
    "This Jupyter Notebook is part of our [A Step-by-Step Guide for Non-Technical Folks on Training Stable Diffusion with a low-cost Cloud GPU](https://learn2train.medium.com/a-step-by-step-guide-for-non-technical-folks-on-training-stable-diffusion-with-a-low-cost-cloud-gpu-344c6b250d64). \n",
    "\n",
    "In this guide, we'll cover the following topics in an interactive way:\n",
    "\n",
    "1. **Fine-tuning a Stable Diffusion base model with a custom dataset**.\n",
    "      \n",
    "2. **Download the training dataset**. \n",
    "\n",
    "3. **Start the training job**\n",
    "    \n",
    "4. **Monitor your sample generations in Weights & Biases (W&B)**. W&B is a free tool used to visualise machine learning experiments. No installation is required as it will be run from a standalone webpage.\n",
    "\n",
    "5. **Training is done** \n",
    "\n",
    "6. **Upload the fine-tuned models to Hugging Face (optional)** so you can re-use them later. Hugging Face is an open-source community for AI experts and enthusiasts. It’s free to use!\n",
    "       \n",
    "7. **Evaluate the fine-tuned checkpoints** to asses its performance.\n",
    "\n",
    "8. **Terminate the GPU instance**. Avoid incurring charges by destroying the GPU instance. \n",
    "\n",
    "\n",
    "### Requirements\n",
    "\n",
    "This interactive tutorial assumes you have:\n",
    "\n",
    "- Setup the training application on a cloud GPU platform according to [this guide](https://learn2train.medium.com/a-step-by-step-guide-for-non-technical-folks-on-training-stable-diffusion-with-a-low-cost-cloud-gpu-344c6b250d64)\n",
    "- A basic understanding of how Jupyter Notebooks work (if you don't check this [cool introduction to Jupyter Notebook demo](https://jupyter.org/try-jupyter/notebooks/?path=notebooks/Intro.ipynb)!)\n",
    "- A reliable internet connection.\n",
    "- An updated browser such as Chrome, Safari, Firefox, etc. \n",
    "- Time to train (it will take about 20 minutes to train the training dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb35beb-c70a-49f7-a369-ceecdc13fd9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Fine-tuning a Stable Diffusion base model with custom data\n",
    "\n",
    "### Fine-tuning using a photographer's image dataset\n",
    "\n",
    "In this notebook tutorial we will train a Stable Diffusion 1.5 base model in the style of a photographer that it doesn't knows very well. By feeding the model with a photographer's image dataset, it should be able to generate pictures in the style of the photographer.  \n",
    "\n",
    "### Bella Kotak\n",
    "\n",
    "In this notebook tutorial, we will fine-tune a Stable Diffusion text-to-image model using Bella Kotak recent artwork. **Bella Kotak** is an award-winning UK-based photographer with a strong, distinctive style.\n",
    "\n",
    "Check her instagram account at [https://www.instagram.com/bellakotak](https://www.instagram.com/bellakotak) ...and be amazed!\n",
    "\n",
    "### Before fine-tuning the base model\n",
    "\n",
    "Hope you checked her portfolio because you need to know how much the base model needs to learn in order to be able to generate decent-looking synthetic images in her artistic style. \n",
    "\n",
    "Since the base model wasn't trained with enough pictures of her artwork, it fails to portray her unique artistic vision. So if we prompt the base Stable Diffusion 1.5 model with **\"a black and white photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak\"**, the base model will struggle to generate a picture that represents her style, or even follow the prompt. \n",
    "  \n",
    "![Bella before](https://drive.google.com/uc?export=view&id=1iUX_aMLQCulbcLMEMbta9GRsPk4VVG-i)\n",
    "\n",
    "### After fine-tuning the base model\n",
    "\n",
    "Thankfully by fine-tuning the base Stable Diffusion model using captioned images, the ability of the base model to generate better-looking pictures based on her style is greatly improved. And even the prompt is better followed. \n",
    "\n",
    "Image below was generated on a fine-tuned Stable Diffusion 1.5 model. It has the same prompt, seed, resolution, and CFG values as the image above!\n",
    "\n",
    "![Bella after](https://drive.google.com/uc?export=view&id=1GgOyCNIFAkjsvkVcYc7U3SlgppLXMJPX)\n",
    "\n",
    "As you can see, it's not perfect -for one thing, it's not exactly black and white- but yet the differences between the non-fine tuned model and the fine-tuned one are rather noticeable. That's the power of training a Stable Diffusion base model with a custom dataset.\n",
    "\n",
    "### Fine-tuning using your own image dataset\n",
    "\n",
    "If you want to use your own images please read the [Preparing and Creating a Dataset for Training Stable Diffusion](https://medium.com/@learn2train/preparing-and-creating-a-dataset-for-training-stable-diffusion-98bdf4854dde) article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ef18e8-b5aa-41fd-8cc8-3ad07d19307f",
   "metadata": {},
   "source": [
    "# 2. Download the training dataset\n",
    "\n",
    "### Download and extract the dataset \n",
    "\n",
    "We are going to download an already prepared training dataset into our GPU instance.\n",
    "\n",
    "A dataset is said to be prepared when every image has a caption describing it. It may or may not include other configuration settings read by the training application. \n",
    "\n",
    "Our image dataset contains 109 images, 109 text files, and 1 tag configuration file (`global.yaml`) that adds a suffix tag to each text file (in this case appends the phrase `in the style of Bella Kotak` to each caption description for each image). For more information about how to create a dataset please refer to chapter II of the tutorial.\n",
    "\n",
    "This is an example of how images and caption files are formatted in our dataset:\n",
    "\n",
    "* `image-name_001.jpg`\n",
    "* `image-name_001.txt`  <= Same filename as the jpg file\n",
    "\n",
    "The text file `image-name_001.txt` contains the caption describing `image-name_001.jpg`, say, for example: `a photo of a woman wearing a floral crown and holding a bouquet of flowers in the style of Bella Kotak`.\n",
    "\n",
    "\n",
    "\n",
    "Running the cell below will download a public ZIP file from Google Drive, extract it and store it in the **input** subfolder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d9f489-5a4d-4816-b66d-a84a2a00993c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests[socks] in /workspace/venv/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /workspace/venv/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: tqdm in /workspace/venv/lib/python3.10/site-packages (from gdown) (4.67.0)\n",
      "Requirement already satisfied: filelock in /workspace/venv/lib/python3.10/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /workspace/venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-5.2.0\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ifk07HeqxHfCCOCvb5oDF-cdxfkfsuq-\n",
      "To: /workspace/EveryDream2trainer/input/dataset.zip\n",
      "100%|██████████████████████████████████████| 20.9M/20.9M [00:00<00:00, 46.8MB/s]\n",
      "\u001b[0m\u001b[01;34mdataset\u001b[0m/\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Install gdown (to be able to download files from Google Drive)\n",
    "!pip install gdown\n",
    "\n",
    "# Download dataset\n",
    "os.makedirs('input', exist_ok=True)\n",
    "path_to_dataset = \"input/dataset.zip\"\n",
    "\n",
    "if not os.path.exists(path_to_dataset):\n",
    "    !gdown 1Ifk07HeqxHfCCOCvb5oDF-cdxfkfsuq- -O input/dataset.zip\n",
    "else:\n",
    "    print(f\"Already downloaded `{path_to_dataset}`\")\n",
    "\n",
    "# Unzip dataset into 'input' folder\n",
    "with zipfile.ZipFile(path_to_dataset, 'r') as zip_ref:\n",
    "    zip_ref.extractall('input/dataset')\n",
    "\n",
    "# Remove zip file\n",
    "os.remove(path_to_dataset)\n",
    "\n",
    "# List input directory\n",
    "%ls input/\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fcd56-0418-4be1-a5c3-38aa679b1aaf",
   "metadata": {},
   "source": [
    "# 3. Start the training job\n",
    "\n",
    "Once our training images and their captions are inside the **input folder** we are ready to train the model. \n",
    "\n",
    "\n",
    "### Training configuration\n",
    "We will override the following default training settings:\n",
    "\n",
    "* **project name**: \"sd1_kotak\" <= Name of the project. It is convenient to name it in a way that identifies it from other training sessions.\n",
    "* **data_root**: \"input\" <= Folder location of the training images\n",
    "* **max epochs**: 60 <= An epoch refers to the one entire passing of training images through the trainer. We are doing 60 entire passes.  \n",
    "* **batch size**: 6 <= Determines the amount of images that are going to be trained every epoch\n",
    "* **sample steps**: 80 <= Determines how frequently samples are generated. In this case we will save every 20 epoch steps.   \n",
    "* **save every n epochs**: 20 <= Checkpoints will be saved every 20 epochs (since we are doing 60 epochs, we will end with 3 checkpoints) \n",
    "* **save ckpt dir**: \"ouput\" <= Folder location of the saved checkpoints\n",
    "* **zero_frequency_noise_ratio**: 0.04 <= This will make dark scenes more realistic  \n",
    "* **optimizer_config**: optimizer-photo.json <= We add an optimiser config file to get better results\n",
    "* **cond_dropout**: 0.0 <= This will prevent the trainer learning images without captions\n",
    "\n",
    "\n",
    "The are more configurations not show here. For a detailed explanation of each check [EveryDream 2 documentation](https://github.com/victorchall/EveryDream2trainer/blob/main/doc/TRAINING.md). \n",
    "\n",
    "### Download the optimizer configuration file\n",
    "\n",
    "Run the following cell to get the optimiser configuration settings to improve our training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f655d4fc-7975-4ca3-bb58-767de9177a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-19 20:29:11--  https://raw.githubusercontent.com/learn2train/l2t-sd/main/notebooks/optimizer-photo.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2039 (2.0K) [text/plain]\n",
      "Saving to: ‘optimizer-photo.json’\n",
      "\n",
      "optimizer-photo.jso 100%[===================>]   1.99K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-11-19 20:29:12 (20.3 MB/s) - ‘optimizer-photo.json’ saved [2039/2039]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/learn2train/l2t-sd/main/notebooks/optimizer-photo.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0141a5-f13f-46f6-a9a5-5bcf6190c0fc",
   "metadata": {},
   "source": [
    "### Set up Weights & Biases (W&B) for monitoring sample generation \n",
    "\n",
    "If you have a W&B account you can use it to track your training progress.  If you don't have one, you can create your W&B account for free at https://wandb.ai/site.\n",
    "\n",
    "You can get your API key from your [User Settings](https://wandb.ai/settings). Paste it in the following cell where it says \"PUT-YOUR-API-KEY-HERE\", keep the double quotes, and then RUN the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f74cc3-d90f-465e-9aaa-0468b83df1ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wandb_token = \"a895d0d585feaa41e14074ea9c5d12864e83810a\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54fbcf-b95e-415c-ad8a-78494ff728bb",
   "metadata": {},
   "source": [
    "The cell above should look like:\n",
    "    \n",
    "`wandb_token = \"28d37291d39f337237291d39f391d39f3\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0a39e-8204-4017-98ef-4ae2451511be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running the training session\n",
    "\n",
    "To start training run the cell below. The cell will start printing its log. Keep scrolling down to monitor the current status of the training session. \n",
    "\n",
    "**IMPORTANT: If you see messages with a red backround, IGNORE THEM as they are only warning messages** \n",
    "\n",
    "The training takes about 20 minutes on a RTX 3090 with 24GB of VRAM. \n",
    "\n",
    "While you wait for the `Training completed` message, watch the samples being generated in Weights & Biases (see cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f73fb86-ebef-41e2-9382-4aa11be84be6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/root/.netrc': No such file or directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38199277f0744aeb983bcb6dd0cfbf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Your branch 'main' is up to date with the remote\n",
      "Loading training config from None.\n",
      "Error on loading training config from None.\n",
      " logging to logs/sd1_kotak-20241119-203020/sd1_kotak-20241119-203020.log\n",
      " Args:\n",
      "{'amp': True,\n",
      " 'aspects': [[512, 512],\n",
      "             [576, 448],\n",
      "             [448, 576],\n",
      "             [640, 384],\n",
      "             [384, 640],\n",
      "             [768, 320],\n",
      "             [320, 768],\n",
      "             [896, 256],\n",
      "             [256, 896],\n",
      "             [1024, 256],\n",
      "             [256, 1024]],\n",
      " 'attn_type': 'sdp',\n",
      " 'batch_size': 6,\n",
      " 'ckpt_every_n_minutes': 1000000000.0,\n",
      " 'clip_grad_norm': None,\n",
      " 'clip_skip': 0,\n",
      " 'cond_dropout': 0.0,\n",
      " 'config': None,\n",
      " 'data_root': 'input',\n",
      " 'disable_amp': False,\n",
      " 'disable_textenc_training': False,\n",
      " 'disable_unet_training': False,\n",
      " 'ema_decay_rate': None,\n",
      " 'ema_device': 'cpu',\n",
      " 'ema_resume_model': None,\n",
      " 'ema_sample_ema_model': False,\n",
      " 'ema_sample_nonema_model': False,\n",
      " 'ema_strength_target': None,\n",
      " 'ema_update_interval': 500,\n",
      " 'embedding_perturbation': 0.0,\n",
      " 'enable_zero_terminal_snr': None,\n",
      " 'flip_p': 0.0,\n",
      " 'gpuid': 0,\n",
      " 'grad_accum': 1,\n",
      " 'gradient_checkpointing': False,\n",
      " 'keep_tags': 0,\n",
      " 'load_settings_every_epoch': None,\n",
      " 'log_step': 25,\n",
      " 'logdir': 'logs',\n",
      " 'loss_type': 'mse_huber',\n",
      " 'lr': None,\n",
      " 'lr_decay_steps': 0,\n",
      " 'lr_scheduler': 'constant',\n",
      " 'lr_warmup_steps': None,\n",
      " 'max_epochs': 30,\n",
      " 'min_snr_gamma': None,\n",
      " 'no_prepend_last': False,\n",
      " 'no_save_ckpt': False,\n",
      " 'optimizer_config': 'optimizer-photo.json',\n",
      " 'plugins': None,\n",
      " 'project_name': 'sd1_kotak',\n",
      " 'pyramid_noise_discount': None,\n",
      " 'rated_dataset': False,\n",
      " 'rated_dataset_target_dropout_percent': 50,\n",
      " 'resolution': 512,\n",
      " 'resume_ckpt': 'learn2train/stable-diffusion-v1-5',\n",
      " 'run_name': None,\n",
      " 'sample_prompts': 'sample_prompts.txt',\n",
      " 'sample_steps': 80,\n",
      " 'save_ckpt_dir': 'output',\n",
      " 'save_ckpts_from_n_epochs': 0,\n",
      " 'save_every_n_epochs': 30,\n",
      " 'save_full_precision': False,\n",
      " 'save_optimizer': False,\n",
      " 'seed': 555,\n",
      " 'shuffle_tags': False,\n",
      " 'timestep_end': 1000,\n",
      " 'timestep_start': 0,\n",
      " 'train_sampler': 'ddpm',\n",
      " 'validation_config': None,\n",
      " 'wandb': True,\n",
      " 'write_schedule': False,\n",
      " 'zero_frequency_noise_ratio': 0.04}\n",
      " Seed: 555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be4e1f712b74b71ac256472f2d99622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897e223bc7f942e7913174c676bd7cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eda9be6d4840e5b379fe6b69047b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/config.json:   0%|          | 0.00/4.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64edb18897804c0e86efb43b14abc494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167c9361df5b470aab1ea5c79ca95cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler/scheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45ab1a5c9034a338ee36cd0df59f7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc677a2ed4c40f3965bfcc0329541dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11a7c72004b427eb06d6531b3a3bc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ature_extractor/preprocessor_config.json:   0%|          | 0.00/520 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf94926b914456b80c26b258dd79841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522ba9722adb49baae56edf9c7e54437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538316f7391b464a8551640aab1c4e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5e5e43c26d47618fe6a3078966e57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06a3a552dc34117b64faaac345e28d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9655cbcd529f48b0a680514865decc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/config.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f381f8e93d364d38bf97fbc4cdf1e4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.bin:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a66177c1b6c4a2ba9ccf5da6929676b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.bin:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c924618c064436bd016565872bcf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--learn2train--stable-diffusion-v1-5/snapshots/c65eab801b6c8d3407a1c6f89569d4465fc1f43f/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--learn2train--stable-diffusion-v1-5/snapshots/c65eab801b6c8d3407a1c6f89569d4465fc1f43f/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/workspace/venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "An error occurred while trying to fetch /root/.cache/huggingface/hub/models--learn2train--stable-diffusion-v1-5/snapshots/c65eab801b6c8d3407a1c6f89569d4465fc1f43f/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /root/.cache/huggingface/hub/models--learn2train--stable-diffusion-v1-5/snapshots/c65eab801b6c8d3407a1c6f89569d4465fc1f43f/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " unet attention_head_dim: 8\n",
      "Inferred yaml: v1-inference.yaml, attn: sd1, prediction_type: epsilon\n",
      "* HuggingFace Downloaded model from learn2train/stable-diffusion-v1-5 to /root/.cache/huggingface/hub/models--learn2train--stable-diffusion-v1-5/snapshots/c65eab801b6c8d3407a1c6f89569d4465fc1f43f.\n",
      "** Using attention yaml file: v1-inference.yaml, is_sd1_attn: True.\n",
      " * Using default (DDPM) noise scheduler for training: ddpm\n",
      "* Using SDP attention *\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msudarshanc00\u001b[0m (\u001b[33msudarshanc00-san-jose-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/EveryDream2trainer/wandb/run-20241119_203110-a8iq2rn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sudarshanc00-san-jose-state-university/sd1_kotak/runs/a8iq2rn0' target=\"_blank\">hopeful-music-2</a></strong> to <a href='https://wandb.ai/sudarshanc00-san-jose-state-university/sd1_kotak' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sudarshanc00-san-jose-state-university/sd1_kotak' target=\"_blank\">https://wandb.ai/sudarshanc00-san-jose-state-university/sd1_kotak</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sudarshanc00-san-jose-state-university/sd1_kotak/runs/a8iq2rn0' target=\"_blank\">https://wandb.ai/sudarshanc00-san-jose-state-university/sd1_kotak/runs/a8iq2rn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* DLMA resolution 512, buckets: [[512, 512], [576, 448], [448, 576], [640, 384], [384, 640], [768, 320], [320, 768], [896, 256], [256, 896], [1024, 256], [256, 1024]]\n",
      " Preloading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preloading: 100%|██████████| 109/109 [00:00<00:00, 3585.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Found 109 files in 'input'\n",
      "No plugins specified\n",
      " * DLMA initialized with 109 images.\n",
      " ** Dataset 'train': 21 batches, num_images: 126, batch_size: 6\n",
      "\n",
      " raw optimizer_config:\n",
      "{'apply_grad_scaler_step_tweaks': True,\n",
      " 'base': {'betas': [0.9, 0.999],\n",
      "          'epsilon': 1e-08,\n",
      "          'lr': 1e-06,\n",
      "          'lr_decay_steps': None,\n",
      "          'lr_scheduler': 'constant',\n",
      "          'lr_warmup_steps': None,\n",
      "          'optimizer': 'adamw8bit',\n",
      "          'weight_decay': 0.01},\n",
      " 'text_encoder_freezing': {'unfreeze_last_n_layers': 6},\n",
      " 'text_encoder_overrides': {'betas': None,\n",
      "                            'epsilon': None,\n",
      "                            'lr': 5e-07,\n",
      "                            'lr_decay_steps': None,\n",
      "                            'lr_scheduler': 'cosine',\n",
      "                            'lr_warmup_steps': None,\n",
      "                            'optimizer': None,\n",
      "                            'weight_decay': None}}\n",
      " Final unet optimizer config:\n",
      "{'betas': [0.9, 0.999],\n",
      " 'epsilon': 1e-08,\n",
      " 'lr': 1e-06,\n",
      " 'lr_decay_steps': 945,\n",
      " 'lr_scheduler': 'constant',\n",
      " 'lr_warmup_steps': 18,\n",
      " 'optimizer': 'adamw8bit',\n",
      " 'weight_decay': 0.01}\n",
      " Final text encoder optimizer config:\n",
      "{'betas': [0.9, 0.999],\n",
      " 'epsilon': 1e-08,\n",
      " 'lr': 5e-07,\n",
      " 'lr_decay_steps': 945,\n",
      " 'lr_scheduler': 'cosine',\n",
      " 'lr_warmup_steps': 18,\n",
      " 'optimizer': 'adamw8bit',\n",
      " 'weight_decay': 0.01}\n",
      " ❄️ freezing embeddings\n",
      " ❄️ freezing text encoder layers 1-6 out of 12 layers total\n",
      "\u001b[94m text encoder weight normal: 305.7\u001b[0m\n",
      "\u001b[94m unet weight normal: 1174.3\u001b[0m\n",
      "\u001b[36m * text encoder optimizer: AdamW8bit (98 parameters) *\u001b[0m\n",
      "\u001b[36m    lr: 5e-07, betas: [0.9, 0.999], epsilon: 1e-08, weight_decay: 0.01 *\u001b[0m\n",
      "\u001b[36m * unet optimizer: AdamW8bit (686 parameters) *\u001b[0m\n",
      "\u001b[36m    lr: 1e-06, betas: [0.9, 0.999], epsilon: 1e-08, weight_decay: 0.01 *\u001b[0m\n",
      " Grad scaler enabled: True (amp mode)\n",
      " * SampleGenerator initialized with 3 prompts, generating samples every 80 training steps, using scheduler 'ddim' with 30 inference steps\n",
      " \u001b[92m** Welcome to EveryDream trainer 2.0!**\u001b[0m\n",
      " (C) 2022-2023 Victor C Hall  This program is licensed under AGPL 3.0 https://www.gnu.org/licenses/agpl-3.0.en.html\n",
      "\n",
      "** Trainer Starting **\n",
      " Pretraining GPU Memory: 4767 / 24564 MB\n",
      " saving ckpts every 1000000000.0 minutes\n",
      " saving ckpts every 30 epochs\n",
      " unet device: cuda:0, precision: torch.float32, training: True\n",
      " text_encoder device: cuda:0, precision: torch.float32, training: True\n",
      " vae device: cuda:0, precision: torch.float16, training: False\n",
      " scheduler: <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>\n",
      " \u001b[32mProject name: \u001b[0m\u001b[92msd1_kotak\u001b[0m\n",
      " \u001b[32mgrad_accum: \u001b[0m\u001b[92m1\u001b[0m\n",
      " \u001b[32mbatch_size: \u001b[0m\u001b[92m6\u001b[0m\n",
      " \u001b[32mepoch_len: \u001b[92m21\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c44d578e6449d0ba5ad1b628b8adf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/EveryDream2trainer/train.py:1011: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  early_timestep_bias = torch.tensor(early_timestep_bias, dtype=torch.float).to(unet.device)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:79 for 3 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:159 for 3 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:239 for 3 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9662ddd58345d7a98dba0ecd4b2467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63cc5e053b6744edac6398a2543a25d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c160b52801427888a0c7b57df19451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a974cd9e5e43414295f35526015509af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:319 for 3 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf14386f0dbf403c88af2c382a595331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80b32fcd513455fadd97f71d55a5a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7532f217e94e4498ae89172a4b92ab64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aff087a5e414dcc99ae624bfe3fbd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:399 for 3 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd489db5da964ecc81d4195c431bbb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78598adbf922416b86a051b17d05af90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18fdb0fde9b4afd94b74e404d2654aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:479 for 3 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3370799b9c24434887a9b32335e1341e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9ef0b6ab2e4e90812dcd7a15aea677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de03f1b5efc747719d242928da07beb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d6e7f7031d46d3bd614ded45b074c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Generating samples at gs:559 for 3 prompts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c9baf4fbd64e86ba84e34ef9b0d6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d496630cdb18442098fb4486b86f651c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc5664e743041e98e28c40a534a0344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Saving diffusers model to logs/sd1_kotak-20241119-203020/ckpts/last-sd1_kotak-ep30-gs00630\n",
      " * Saving SD model to output/last-sd1_kotak-ep30-gs00630.safetensors\n",
      "\u001b[36mTraining complete\u001b[0m\n",
      "Total training time took 6.93 minutes, total steps: 630\n",
      "Average epoch time: 0.22 minutes\n",
      "\u001b[97m ***************************\u001b[0m\n",
      "\u001b[97m **** Finished training ****\u001b[0m\n",
      "\u001b[97m ***************************\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get the wandb token\n",
    "wandb_settings = \"\"\n",
    "if wandb_token:\n",
    "  !rm /root/.netrc\n",
    "  !wandb login $wandb_token\n",
    "  wandb_settings = \"--wandb\"\n",
    "\n",
    "# Start the training\n",
    "\n",
    "%run train.py --resume_ckpt \"learn2train/stable-diffusion-v1-5\" \\\n",
    "$wandb_settings \\\n",
    "--project_name \"sd1_kotak\" \\\n",
    "--data_root \"input\" \\\n",
    "--max_epochs 30 \\\n",
    "--sample_steps 80 \\\n",
    "--batch_size 6 \\\n",
    "--save_every_n_epochs 30 \\\n",
    "--zero_frequency_noise_ratio 0.04 \\\n",
    "--cond_dropout 0.0 \\\n",
    "--optimizer_config optimizer-photo.json \\\n",
    "--save_ckpt_dir \"output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0be597-abb7-4f52-b29e-11f18c387edb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Watch your samples in Weights & Biases while training is running\n",
    "\n",
    "### W&B dashboard\n",
    "\n",
    "Go to the [W&B dashboard](https://wandb.ai/home) in another browser tab. You will see your training run in your home page. \n",
    "\n",
    "Click on your training run to check the samples being generated. They should give you an idea how good/bad your model learning progress is going. You should stop the training once you are satisfied with the results you are seeing.\n",
    "\n",
    "Samples come in three. That is because each sample generated uses different CFG values (1, 4 and 7).\n",
    "\n",
    "![W&B](https://drive.google.com/uc?export=view&id=1G1fmv5uFN_pk57jBhmD7SVes4at-uv4C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd913f-3c0d-4014-b6e1-6b2fd9cfc2b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. Training is finished\n",
    "\n",
    "Once the training is done you should see the following messages:\n",
    "\n",
    "![Training is finished](https://drive.google.com/uc?export=view&id=1WXwNcHaKStpuusvReueriEJXsl3rLWRM)\n",
    "\n",
    "\n",
    "That was it! The base model has been updated and you are now left with checkpoints.\n",
    "\n",
    "Before terminating the GPU instance, you could save the checkpoints to your computer, upload them to your Hugging Face repository, or transfer them to another storage plaform such as AWS S3.\n",
    "\n",
    "\n",
    "I strongly recommend that you **DO NOT** download the checkpoints to your computer because of the time it could take to save them. It's always better and faster to transfer them to AWS S3, Cloudflare R2, or to your Hugging Face as you'll see next.  \n",
    "\n",
    "If you insist in saving them to your computer you can use the file explorer on the left panel, double click in the **output** folder, select the checkpoints you want to download and click download file.  \n",
    "\n",
    "![output folder](https://drive.google.com/uc?export=view&id=1owrfdiPrJy7M0M7wkI9c5N_rBM8ukbx5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0982c-6029-4fc1-b161-f71c18680261",
   "metadata": {},
   "source": [
    "# 6. Upload your checkpoints to Hugging Face (optional but highly recommended)\n",
    "\n",
    "If you aren't saving your checkpoints to your computer, you could save them to your Hugging Face repository instead. That way you can easily re-use or share them. \n",
    "\n",
    "### Get a Hugging Face token\n",
    "\n",
    "If you haven't got one yet, have a look at [How to Host Stable Diffusion Checkpoints on Hugging Face for Free](https://learn2train.medium.com/a-step-by-step-guide-to-host-stable-diffusion-checkpoints-on-hugging-face-for-free-2098d0c18a01)\n",
    "\n",
    "### Log-in into your account \n",
    "\n",
    "Run the cell below and paste your **Hugging Face write token** into the prompt that will pop-up to log into your account (no need to check the **git credentials** box). You need to login to Hugging Face to be able to upload data into your repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b7db86-1294-46ff-8e7a-5215f3738584",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081b4a52-97ab-4d70-8db7-43d1203135fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ab8c5779314e9e9d575966e15866e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Log in to Hugging Face\n",
    "\n",
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "import os\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24eee3d-f5df-45f3-9acc-ee0206cfe6b1",
   "metadata": {},
   "source": [
    "### Upload checkpoints to your model repository\n",
    "\n",
    "Make sure you are **logged in** to Hugging Face running the above login cell first.\n",
    "\n",
    "Use the cell below to upload one or more checkpoints to your personal Hugging Face repository. You should already be authorized to interact with Hugging Face if you ran the cell above.\n",
    "\n",
    "When you run the cell below, a box will show up and you need to  **CLICK** to select which `.safetensors` file are marked for upload. This allows you to select which ones to upload.  If you don't click of the ckpts, nothing will happen.\n",
    "\n",
    "You will also be required to fill-in your username and your repository name:\n",
    "* Hugging Face username: **your username** (look in [HuggingFace account page](https://huggingface.co/settings/account)).\n",
    "* Hugging Face repository name: **your repo name**\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "**If your Hugging Face account is brand new upload only 3 checkpoint files**. For safety reasons, Hugging Face limits the amount of files a new user can make. If you try to upload more than 3 checkpoint files you'll probably get a warning tell you to wait 24 hours to keep uploading. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9df5e1a-3c68-41c0-a4ed-ea0abcd19858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a09aa3303704e3cbd9a743df6c240b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(SelectMultiple(layout=Layout(width='600px'), options=('output/last-sd1_kotak-ep30-gs00630.safet…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell after reading the instructions of the cell above. \n",
    "\n",
    "import glob\n",
    "import os\n",
    "from huggingface_hub import HfApi\n",
    "from ipywidgets import *\n",
    "\n",
    "all_ckpts = [f for f in glob.glob(\"output/*.safetensors\")]\n",
    "  \n",
    "ckpt_picker = SelectMultiple(options=all_ckpts, layout=Layout(width=\"600px\")) \n",
    "hfuser = Text(placeholder='Hugging Face username')\n",
    "hfrepo = Text(placeholder='Hugging Face repository name')\n",
    "\n",
    "api = HfApi()\n",
    "upload_btn = Button(description='Upload')\n",
    "out = Output()\n",
    "\n",
    "def upload_ckpts(_):\n",
    "    repo_id=f\"{hfuser.value or hfuser.placeholder}/{hfrepo.value or hfrepo.placeholder}\"\n",
    "    with out:\n",
    "        if ckpt_picker is None or len(ckpt_picker.value) < 1:\n",
    "            print(\"Nothing selected for upload, make sure to click one of the ckpt files in the list, or, you have no ckpt files in the current directory.\")\n",
    "        for ckpt in ckpt_picker.value:\n",
    "            print(f\"Uploading to HF: huggingface.co/{repo_id}/{ckpt}\")\n",
    "            response = api.upload_file(\n",
    "                path_or_fileobj=ckpt,\n",
    "                path_in_repo=ckpt,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=None,\n",
    "                create_pr=1,\n",
    "            )\n",
    "            display(response)\n",
    "        print(\"DONE\")\n",
    "\n",
    "upload_btn.on_click(upload_ckpts)\n",
    "box = VBox([ckpt_picker, HBox([hfuser, hfrepo]), upload_btn, out])\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4a28a-2488-4216-b431-fcc3e9d1de9e",
   "metadata": {},
   "source": [
    "### Save the uploads to your model repository\n",
    "\n",
    "To actually save the uploaded checkpoints to your repo, go back to your Hugging Face model repository and click the **Community** tab. You'll see a list of one or more checkpoints. Go one by one and click **Merge** to save them to your model repository:\n",
    "\n",
    "![Merge](https://drive.google.com/uc?export=view&id=1zyOcOq9uABW1dO69pNYenvsag1C7asyc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a00d16-9b84-492f-8e6a-defe71e82b43",
   "metadata": {},
   "source": [
    "# 7. Evaluate your fine-tuned checkpoints\n",
    "\n",
    "\n",
    "### Test inference on your checkpoints\n",
    "\n",
    "To recap: Training is over and you are left with model checkpoints (safetensor files). These checkpoints are updated fine-tuned models saved at different times during the training session. \n",
    "\n",
    "The main idea here is to evaluate each of your checkpoints to find the ones that generate the output you like the most.  \n",
    "\n",
    "Run the following cell to display a mini text-to-image generator. You can choose any checkpoint -or all of them- and set inference parameters such as **prompt, steps, CFG, resolution and seed**.\n",
    "\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb1a8cd-6a04-44e5-a770-c23ee247ce82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75403ee35cda46b5a4381ce737ea7736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Checkpoint:', layout=Layout(width='600px'), options=('./logs/sd1_kotak-20…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, DDIMScheduler, DDPMScheduler, PNDMScheduler, EulerAncestralDiscreteScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "\n",
    "checkpoints_ts = []\n",
    "for root, dirs, files in os.walk(\".\"):\n",
    "        for file in files:\n",
    "            if os.path.basename(file) == \"model_index.json\":\n",
    "                ts = os.path.getmtime(os.path.join(root,file))\n",
    "                ckpt = root\n",
    "                checkpoints_ts.append((ts, root))\n",
    "\n",
    "checkpoints = [ckpt for (_, ckpt) in sorted(checkpoints_ts, reverse=True)]\n",
    "full_width = Layout(width='600px')\n",
    "half_width = Layout(width='300px')\n",
    "\n",
    "checkpoint = Dropdown(options=checkpoints, description='Checkpoint:', layout=full_width)\n",
    "prompt = Textarea(value='a photo of ', description='Prompt:', layout=full_width)\n",
    "height = IntSlider(value=512, min=256, max=768, step=32, description='Height:', layout=half_width)\n",
    "width = IntSlider(value=512, min=256, max=768, step=32, description='Width:', layout=half_width)\n",
    "cfg = FloatSlider(value=7.0, min=0.0, max=14.0, step=0.2, description='CFG Scale:', layout=half_width)\n",
    "steps = IntSlider(value=30, min=10, max=100, description='Steps:', layout=half_width)\n",
    "seed = IntText(value=-1, description='Seed:', layout=half_width)\n",
    "generate_btn = Button(description='Generate', layout=full_width)\n",
    "out = Output()\n",
    "\n",
    "def generate(_):\n",
    "    with out:\n",
    "        clear_output()\n",
    "        display(f\"Loading model {checkpoint.value}\")\n",
    "        actual_seed = seed.value if seed.value != -1 else random.randint(0, 2**30)\n",
    "\n",
    "        text_encoder = CLIPTextModel.from_pretrained(checkpoint.value, subfolder=\"text_encoder\")\n",
    "        vae = AutoencoderKL.from_pretrained(checkpoint.value, subfolder=\"vae\")\n",
    "        unet = UNet2DConditionModel.from_pretrained(checkpoint.value, subfolder=\"unet\")\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(checkpoint.value, subfolder=\"tokenizer\", use_fast=False)\n",
    "        scheduler = DDIMScheduler.from_pretrained(checkpoint.value, subfolder=\"scheduler\")\n",
    "        text_encoder.eval()\n",
    "        vae.eval()\n",
    "        unet.eval()\n",
    "\n",
    "        text_encoder.to(\"cuda\")\n",
    "        vae.to(\"cuda\")\n",
    "        unet.to(\"cuda\")\n",
    "\n",
    "        pipe = StableDiffusionPipeline(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            tokenizer=tokenizer,\n",
    "            unet=unet,\n",
    "            scheduler=scheduler,\n",
    "            safety_checker=None, # save vram\n",
    "            requires_safety_checker=None, # avoid nag\n",
    "            feature_extractor=None, # must be none of no safety checker\n",
    "        )\n",
    "\n",
    "        pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "        \n",
    "        print(inspect.cleandoc(f\"\"\"\n",
    "              Prompt: {prompt.value}\n",
    "              Resolution: {width.value}x{height.value}\n",
    "              CFG: {cfg.value}\n",
    "              Steps: {steps.value}\n",
    "              Seed: {actual_seed}\n",
    "              \"\"\"))\n",
    "        with autocast(\"cuda\"):\n",
    "            image = pipe(prompt.value, \n",
    "                generator=torch.Generator(\"cuda\").manual_seed(actual_seed),\n",
    "                num_inference_steps=steps.value, \n",
    "                guidance_scale=cfg.value,\n",
    "                width=width.value,\n",
    "                height=height.value\n",
    "            ).images[0]\n",
    "        del pipe\n",
    "        gc.collect()\n",
    "        with torch.cuda.device(\"cuda\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "        display(image)\n",
    "            \n",
    "generate_btn.on_click(generate)\n",
    "box = VBox(\n",
    "    children=[\n",
    "        checkpoint, prompt, \n",
    "        HBox([VBox([width, height]), VBox([steps, cfg])]), \n",
    "        seed, \n",
    "        generate_btn, \n",
    "        out]\n",
    ")\n",
    "\n",
    "\n",
    "display(box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa733fb-d112-4d2f-9771-5fc8ac11e0c8",
   "metadata": {},
   "source": [
    "# 8. Terminate your GPU instance when you are done\n",
    "\n",
    "Don't forget to terminate your cloud GPU instance once you are done evaluating your checkpoints, otherwise you will be still charged. Check the last section of the previous chapter to see how to terminate your instance. \n",
    "\n",
    "Note that once you terminate your instance **Jupyter Lab** will stop working. If you want to use it again you'll have to start a new training session on the same or difference GPU instance, and start all over again. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2e677f113ff5b533036843965d6e18980b635d0aedc1c5cebd058006c5afc92a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
